{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scrapper.user_stats_scrapper import UserStatsScrapper\n",
    "from scrapper.user_friends_scrapper import UserFriendsScrapper\n",
    "from scrapper.network.tor_proxy import TorProxy\n",
    "from scrapper.network.list_proxy import ListProxy\n",
    "from scrapper.scrapping_status import Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(thread_id, msg):\n",
    "    with open(f\"logs/{thread_id}.txt\", \"a\") as f:\n",
    "        f.write(msg + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_thread(df, df_lock, df_save_path, saving_interval=300):\n",
    "    i=1\n",
    "    while True:\n",
    "        time.sleep(saving_interval)\n",
    "        df_lock.acquire()\n",
    "        try:\n",
    "            df.to_pickle(df_save_path)\n",
    "            print(\"DF saved at {:03d}: {} entries\".format(i, df.shape[0]))\n",
    "        finally:\n",
    "            df_lock.release()\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tor_proxies(num_threads, starting_port):\n",
    "    return [TorProxy(starting_port + 2*i, starting_port + 2*i + 1) for i in range(num_threads)]\n",
    "\n",
    "def get_txt_proxies(list_path, proxy_type, num_threads):\n",
    "    proxy_ips = []\n",
    "    with open(list_path, \"r\") as f:\n",
    "        proxy_ips = [ip.strip() for ip in f.readlines()]\n",
    "    np.random.shuffle(proxy_ips)\n",
    "    proxy_ips = np.array_split(proxy_ips, num_threads)\n",
    "    return [ListProxy(ips, proxy_type) for ips in proxy_ips]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unvisited_users(seen_users):\n",
    "    all_users = pd.read_pickle(\"user_friends.pkl\")\n",
    "    all_users = all_users[all_users.status == Status.VISITED]\n",
    "    all_users = all_users[all_users.friends.apply(len) > 0]\n",
    "    all_users = set(all_users.friends.explode().unique())\n",
    "    return np.array(list(all_users - set(seen_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_user_friends(num_threads, user_ids_thread_limit = 200):\n",
    "    user_friends_lock = threading.Lock()\n",
    "    df_save_path = \"user_friends.pkl\"\n",
    "    user_friends_df = pd.read_pickle(df_save_path)\n",
    "    \n",
    "    def scrap_user_friends_thread(thread_id, user_ids, proxy):\n",
    "        time.sleep(2 * random.random())\n",
    "        auth_cookie = {'untappd_user_v3_e': '59884cc5903a2ad0d4a2707a8caf891d9ac17e0c016977b66432c1e7ae6b2d5667ed6a177cccf18861870eb1c0d6b333888d6d0c01ae69b45e5dcd0c5bb00d1edReLZMP%2Fi3XSY3q3FUNdC6FMVPkz3hUGk%2FFPBfVStfaamglZ0wJMZczAFofaAewWTdWi%2BCC260FZ1uGrzfRWGg%3D%3D'}\n",
    "        i = 0\n",
    "        \n",
    "        for user_id in user_ids:\n",
    "            log(thread_id, f\"Scrapping {i}th user: {user_id}:\")\n",
    "            status, friends = UserFriendsScrapper(user_id).scrap(auth_cookie, proxy)\n",
    "            \n",
    "            new_entry = {}\n",
    "            new_entry[\"status\"]  = status\n",
    "            new_entry[\"friends\"] = friends\n",
    "            \n",
    "            user_friends_lock.acquire()\n",
    "            try:\n",
    "                user_friends_df.loc[user_id] = new_entry\n",
    "            finally:\n",
    "                user_friends_lock.release()\n",
    "                log(thread_id, f\"    done with status: {status}\")\n",
    "                i += 1\n",
    "        return\n",
    "    \n",
    "    new_users = find_unvisited_users(user_friends_df.index.unique())\n",
    "    if new_users.size > (user_ids_thread_limit * num_threads):\n",
    "        new_users = new_users[0:user_ids_thread_limit * num_threads]\n",
    "    \n",
    "    user_splits = np.array_split(new_users, num_threads)\n",
    "    proxies = get_txt_proxies(\"proxies/fineproxy_socks5.txt\", \"socks5\", num_threads)\n",
    "    #proxies = get_tor_proxies(num_threads, starting_port=10000)\n",
    "    \n",
    "    for i in range(num_threads):\n",
    "        threading.Thread(target=scrap_user_friends_thread, args=(i, user_splits[i], proxies[i],)).start()\n",
    "    threading.Thread(target=saving_thread, args=(user_friends_df, user_friends_lock, df_save_path,)).start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lets go\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:ListProxy - renewing IP: removed 83.142.52.20:1085 for 194.104.10.161:1085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF saved at 001: 457341 entries\n"
     ]
    }
   ],
   "source": [
    "scrap_user_friends(100, 5000)\n",
    "print(\"lets go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**17/09:**  \n",
    "12000 at 13:01  \n",
    "50400 at 14:34  \n",
    "84812 at 17:15  \n",
    "94332 at 18:14  \n",
    "104519 at 19:41  \n",
    "128542 at 23:28  \n",
    "136000 at 00:00  \n",
    "  \n",
    "**18/09:**  \n",
    "restarted at 10h15  \n",
    "156551 at 12:50 <- threading was unsuccessful  \n",
    "180000 at 16:53  \n",
    "201587 at 19:45  \n",
    "224700 at 23:45\n",
    "\n",
    "**19/09** (tor will be tried today):  \n",
    "restarted at 9:56  \n",
    "232000 at 11:45 pause until tor implementation  \n",
    "restarted at 13:50 with 50 tor workers (4 workers crashed in 10min, will stop experiment at 10 crashes)  \n",
    "272000 at 14:00 (YAAAYYY!!!)  \n",
    "283600 at 14:06  \n",
    "317500 at 14:20 stopped the experiment  \n",
    "restarted at 14:30 with 100 workers but bigger delay to avoid ip-banning  \n",
    "328000 at 14:35  \n",
    "474000 at 16:15  \n",
    "600000 at 18:20 stopped experiment after 11 ip ban and 20 worker crashes  \n",
    "  \n",
    "**20/09**:  \n",
    "restarted at 0:00  \n",
    "913000 at 10:00 stopped (10% found!!!!)  \n",
    "restarted at 12:00  \n",
    "1010687 at 13:10  \n",
    "  \n",
    "**24/09**:  \n",
    "restarted at 12:00 with 1500000  \n",
    "1600000 at 14:00 stopped  \n",
    "restarted at 15:15  \n",
    "1670000 at 18:12 stopped  \n",
    "\n",
    "**25/09**:  \n",
    "restarted at 12:20 with fineproxy lists (100 simultaneous requests)  \n",
    "1710000 at start  \n",
    "1787000 at 13:45 restarted with 200 simultaneous requests (1300users/5min)  \n",
    "pause at 16:25, going with 500 simultaneous requests  \n",
    "1963000 at 21:20 stopped  \n",
    "restarted at 22:10  \n",
    "1992000 at 0:00  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
